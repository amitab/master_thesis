{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 images each with 3 channels, dimensions 224 x 224 \n",
    "imgs = np.random.rand(10,3,224,224)\n",
    "header = \"{},{},{},{}\".format(*imgs.shape)\n",
    "np.savetxt(\"images_10_3_224_224.np\", imgs.flatten(), header=header)\n",
    "\n",
    "# 64 kernels, each with 3 channels, dimensions 7 x 7\n",
    "kernel = np.random.rand(64,3,7,7)\n",
    "header = \"{},{},{},{}\".format(*kernel.shape)\n",
    "np.savetxt(\"kernel_64_3_7_7.np\", kernel.flatten(), header=header)\n",
    "\n",
    "# bias vector of length 64\n",
    "bias = np.random.rand(64, 1)\n",
    "header = \"{}, {}\".format(*bias.shape)\n",
    "np.savetxt(\"bias_64.np\", bias.flatten(), header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.loadtxt(\"images_1.np\").reshape(1,2,6,6)\n",
    "kernel = np.loadtxt(\"kernel_3.np\").reshape(3,2,3,3)\n",
    "bias = np.loadtxt(\"bias_3.np\").reshape(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.15955306, 0.84906901, 0.54561362, 0.89604376, 0.37660462,\n",
       "          0.32604895],\n",
       "         [0.88536291, 0.72544014, 0.30213657, 0.24391796, 0.8367923 ,\n",
       "          0.00677764],\n",
       "         [0.68139817, 0.45835189, 0.40025061, 0.12925077, 0.38525406,\n",
       "          0.56888845],\n",
       "         [0.61425965, 0.93827786, 0.61741657, 0.25136994, 0.41869429,\n",
       "          0.27364   ],\n",
       "         [0.38343472, 0.52306167, 0.9823107 , 0.95366812, 0.25909526,\n",
       "          0.02918973],\n",
       "         [0.5683895 , 0.60234376, 0.1621629 , 0.32172377, 0.34041692,\n",
       "          0.05159952]],\n",
       "\n",
       "        [[0.09849916, 0.16914041, 0.89250968, 0.5272783 , 0.87366801,\n",
       "          0.12653044],\n",
       "         [0.53313813, 0.95166385, 0.4389646 , 0.65317807, 0.43628923,\n",
       "          0.90843706],\n",
       "         [0.8469585 , 0.43609608, 0.55670027, 0.54856218, 0.8700725 ,\n",
       "          0.60629741],\n",
       "         [0.36392068, 0.24211918, 0.48511723, 0.44674531, 0.06507306,\n",
       "          0.71049912],\n",
       "         [0.49205998, 0.51518035, 0.65234204, 0.20369967, 0.5902329 ,\n",
       "          0.84236075],\n",
       "         [0.15014032, 0.92294339, 0.91016256, 0.91172   , 0.00540243,\n",
       "          0.91656066]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.96844024, 0.91677037, 0.20084168],\n",
       "         [0.51737026, 0.84232667, 0.16916628],\n",
       "         [0.00337454, 0.52294908, 0.97996873]],\n",
       "\n",
       "        [[0.49270656, 0.28457836, 0.4204916 ],\n",
       "         [0.38666854, 0.74145295, 0.77173518],\n",
       "         [0.16400922, 0.20366011, 0.53234345]]],\n",
       "\n",
       "\n",
       "       [[[0.20287251, 0.63202806, 0.87074152],\n",
       "         [0.50011759, 0.21778542, 0.02711159],\n",
       "         [0.69487962, 0.59547085, 0.4803164 ]],\n",
       "\n",
       "        [[0.3663493 , 0.1255844 , 0.77317302],\n",
       "         [0.00984027, 0.37000069, 0.05565784],\n",
       "         [0.37260327, 0.56026268, 0.10974279]]],\n",
       "\n",
       "\n",
       "       [[[0.37104108, 0.65179618, 0.47142603],\n",
       "         [0.17087244, 0.01127146, 0.12772858],\n",
       "         [0.86150483, 0.8443549 , 0.55282022]],\n",
       "\n",
       "        [[0.71372867, 0.95544612, 0.34553299],\n",
       "         [0.09791882, 0.69473835, 0.28326671],\n",
       "         [0.32258273, 0.91501908, 0.207473  ]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51915282, 0.34316147, 0.91806065])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.Conv2d(2, 3, 3, stride=1, padding=0, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weight = torch.nn.Parameter(torch.tensor(kernel))\n",
    "m.bias = torch.nn.Parameter(torch.tensor(bias.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5.5627, 5.2637, 5.5086, 5.8179],\n",
       "          [6.1722, 4.9657, 4.2127, 5.1173],\n",
       "          [6.1327, 5.5818, 4.2886, 3.9523],\n",
       "          [5.6042, 5.8132, 4.8795, 4.4901]],\n",
       "\n",
       "         [[4.6851, 4.0191, 4.0644, 3.3213],\n",
       "          [4.1573, 3.9146, 3.5811, 3.2852],\n",
       "          [4.1568, 4.2590, 4.1325, 3.5096],\n",
       "          [4.2057, 4.0203, 3.4149, 3.3140]],\n",
       "\n",
       "         [[5.3470, 5.4767, 5.6353, 5.4309],\n",
       "          [6.1220, 5.5910, 5.0273, 4.7556],\n",
       "          [5.4880, 5.7419, 5.4067, 5.1275],\n",
       "          [5.6648, 5.5148, 4.6167, 3.9603]]]], dtype=torch.float64,\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = m(torch.tensor(imgs))\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 112, 112])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
